<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="non-parametric-density-estimation.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>2.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.2</b> Geometric interpretation</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#hat-matrix"><i class="fa fa-check"></i><b>2.3</b> Hat matrix</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression-vs.simple-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple regression vs. simple regression</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties"><i class="fa fa-check"></i><b>2.5</b> Properties</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#tests"><i class="fa fa-check"></i><b>2.6</b> Tests</a></li>
<li class="chapter" data-level="2.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.7</b> Diagnostics</a></li>
<li class="chapter" data-level="2.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>2.8</b> Generalized least squares</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Non-parametric Density Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-histogramm"><i class="fa fa-check"></i><b>3.1</b> The Histogramm</a></li>
<li class="chapter" data-level="3.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#kernels"><i class="fa fa-check"></i><b>3.2</b> Kernels</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-naive-estimator"><i class="fa fa-check"></i><b>3.2.1</b> The naive Estimator</a></li>
<li class="chapter" data-level="3.2.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-kernels"><i class="fa fa-check"></i><b>3.2.2</b> Other Kernels</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-bandwidth"><i class="fa fa-check"></i><b>3.3</b> The Bandwidth</a></li>
<li class="chapter" data-level="3.4" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#bringing-it-all-together"><i class="fa fa-check"></i><b>3.4</b> Bringing it all together</a></li>
<li class="chapter" data-level="3.5" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-density-estimators"><i class="fa fa-check"></i><b>3.5</b> Other Density Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#higher-dimensions"><i class="fa fa-check"></i><b>3.6</b> Higher Dimensions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.6.1</b> The Curse of Dimensionality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>4</b> Non-parametric Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#alternative-interpretation"><i class="fa fa-check"></i><b>4.1</b> Alternative Interpretation</a></li>
<li class="chapter" data-level="4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#the-bandwidth-1"><i class="fa fa-check"></i><b>4.2</b> The Bandwidth</a></li>
<li class="chapter" data-level="4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#hat-matrix-1"><i class="fa fa-check"></i><b>4.3</b> Hat Matrix</a></li>
<li class="chapter" data-level="4.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>4.4</b> Degrees of Freedom</a><ul>
<li class="chapter" data-level="4.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#applications"><i class="fa fa-check"></i><b>4.4.1</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#inference"><i class="fa fa-check"></i><b>4.5</b> Inference</a></li>
<li class="chapter" data-level="4.6" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#local-polynomial-estimator"><i class="fa fa-check"></i><b>4.6</b> Local Polynomial Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementations"><i class="fa fa-check"></i><b>5.3</b> Implementations</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#isolation-of-each-cross-validation-sample"><i class="fa fa-check"></i><b>5.7</b> Isolation of each cross validation sample</a></li>
<li class="chapter" data-level="5.8" data-path="cross-validation.html"><a href="cross-validation.html#examples-with-r"><i class="fa fa-check"></i><b>5.8</b> Examples with R</a><ul>
<li class="chapter" data-level="5.8.1" data-path="cross-validation.html"><a href="cross-validation.html#application-1-estimating-the-generalization-error"><i class="fa fa-check"></i><b>5.8.1</b> Application 1: Estimating the generalization error</a></li>
<li class="chapter" data-level="5.8.2" data-path="cross-validation.html"><a href="cross-validation.html#application-2-parameter-tuning"><i class="fa fa-check"></i><b>5.8.2</b> Application 2: Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression-1"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
<li class="chapter" data-level="8.2.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.3</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.4</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.4.1</b> Proejction Pursuit Example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>8.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#prediction-given-partitioning"><i class="fa fa-check"></i><b>8.5.1</b> Prediction given Partitioning</a></li>
<li class="chapter" data-level="8.5.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#assumptions-on-the-patritions"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions on the Patritions</a></li>
<li class="chapter" data-level="8.5.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#algorithm"><i class="fa fa-check"></i><b>8.5.3</b> Algorithm</a></li>
<li class="chapter" data-level="8.5.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#backward-deletion-pruning"><i class="fa fa-check"></i><b>8.5.4</b> Backward Deletion / Pruning</a></li>
<li class="chapter" data-level="8.5.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>8.5.5</b> Pros and Cons of Trees</a></li>
<li class="chapter" data-level="8.5.6" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#random-forests"><i class="fa fa-check"></i><b>8.5.6</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html"><i class="fa fa-check"></i><b>9</b> Variable Selection - Ridge Regression an Lasso</a><ul>
<li class="chapter" data-level="9.1" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#ridge-regression"><i class="fa fa-check"></i><b>9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="9.2" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#lasso"><i class="fa fa-check"></i><b>9.2</b> Lasso</a></li>
<li class="chapter" data-level="9.3" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#extensions"><i class="fa fa-check"></i><b>9.3</b> Extensions</a><ul>
<li class="chapter" data-level="9.3.1" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#elastic-net"><i class="fa fa-check"></i><b>9.3.1</b> Elastic Net</a></li>
<li class="chapter" data-level="9.3.2" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#relaxed-lasso"><i class="fa fa-check"></i><b>9.3.3</b> Relaxed Lasso</a></li>
<li class="chapter" data-level="9.3.4" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#sparse-group-lasso"><i class="fa fa-check"></i><b>9.3.4</b> (Sparse) Group Lasso</a></li>
<li class="chapter" data-level="9.3.5" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#oracle-properties"><i class="fa fa-check"></i><b>9.3.5</b> Oracle Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and Boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#subagging"><i class="fa fa-check"></i><b>10.2</b> Subagging</a></li>
<li class="chapter" data-level="10.3" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#l_2-boosting"><i class="fa fa-check"></i><b>10.3</b> <span class="math inline">\(L_2\)</span>-Boosting</a></li>
<li class="chapter" data-level="10.4" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#some-unfinished-stuff"><i class="fa fa-check"></i><b>10.4</b> Some unfinished stuff</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="round-up.html"><a href="round-up.html"><i class="fa fa-check"></i><b>11</b> Round up</a><ul>
<li class="chapter" data-level="11.1" data-path="round-up.html"><a href="round-up.html#comparing-models"><i class="fa fa-check"></i><b>11.1</b> Comparing models</a></li>
<li class="chapter" data-level="11.2" data-path="round-up.html"><a href="round-up.html#exercises-take-aways"><i class="fa fa-check"></i><b>11.2</b> Exercises Take-aways</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Multiple Linear Regression</h1>
<p>This chapter has not yet been summarized completely.</p>
<p>Since liner models are well known (at least to me), the chapter is kept very brief.</p>
<p>The goal of linear modelling is</p>
<ul>
<li>A good fit: Explain a lot of variance and keep the errors small.</li>
<li>Good parameter estimates.</li>
<li>Prediction: Predict new values.</li>
<li>Inference: Say something about whether a variable has <em>an influence</em> or not on the response via significance levels and confidence intervals.</li>
</ul>
<p>Note that a good (in-sample) fit conflicts with good prediction out-of-sample.</p>
<p>Linear models are stochastic, since <span class="math inline">\(\epsilon\)</span> is stochastic and hence the response variable <span class="math inline">\(Y_i\)</span> also. We can assume that the predictors are non-random.</p>
<p>Linear models are called linear because they are linear in the coefficients, not linear in the predictors The predictors can be transformed arbitrary.</p>
<p>Note the difference between the residuals <span class="math inline">\(r_i\)</span> and the errors <span class="math inline">\(\epsilon_i\)</span>. The variance of the error can be estimated with</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n - p} \sum\limits_{i = 1}^n {r_i}^2\]</span> The solution linear models is computed via ordinary least squares and can be derived analytically.</p>
<p><span class="math display">\[\hat{\beta} = (X&#39;X)^{-1}X&#39;Y\]</span> For numerical stability reasons, do not compute the inverse of a matrix if possible, but do a QR decomposition instead. Here, we can do that by using the normal equations <span class="math display">\[
\begin{split}
\hat{\beta} &amp; = (X&#39;X)^{-1}X&#39;Y\;\;\;\;\, | \times (X&#39;X) \;\; \text{from left} \\
(X&#39;X)\hat{\beta} &amp;= X&#39;Y \;\;\; \;\;\;  \;\;\; \;\;\; \;\;\;  | X = QR \;\;\; \text{whereas} \;\;Q \;\; \text{is orthogonal} \\
R&#39;R\hat{\beta} &amp;= X&#39;Y \;\;\; \;\;\;  \;\;\; \;\;\; \;\;\; | \;\; \text{set} \;\; c= R\beta \\
R&#39;c &amp; = X&#39;Y \\
\end{split}
\]</span></p>
<p>Since <span class="math inline">\(R&#39;\)</span> is a lower triangular matrix, you can now solve for <span class="math inline">\(c\)</span> recursively.</p>
<p><span class="math display">\[\begin{pmatrix}
\cdots  &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
  &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
  &amp; &amp; \cdots &amp; \cdots &amp; \cdots \\
  &amp; &amp; &amp; \cdots &amp; \cdots \\
  \large 0 &amp; &amp; &amp; &amp; r_{nn}
 \end{pmatrix} \times 
 \begin{pmatrix} 
 c_1 \\
  \vdots \\
\vdots \\\vdots \\
 c_n \\
 \end{pmatrix} 
 = (X&#39;Y)_{n \times 1}\]</span> First, solve <span class="math inline">\(c_n\)</span> using <span class="math inline">\(r_{nn} c_n = (X&#39;Y)_{nn}\)</span>, then for <span class="math inline">\(c_{n-1}\)</span> etc. when <span class="math inline">\(c\)</span> is solved, solve <span class="math display">\[ R\beta = c\]</span></p>
<p>Which has the same structure.</p>
<div id="assumptions-of-the-linear-model" class="section level2">
<h2><span class="header-section-number">2.1</span> Assumptions of the linear model</h2>
<ol style="list-style-type: decimal">
<li>The functional form of the model is correct <span class="math inline">\(\mathbb{E}[\epsilon_i] = 0\)</span>. If violated, you cannot use linear models.</li>
<li>The <span class="math inline">\(x_i\)</span>s are exact.</li>
<li>The variance of the errors is constant (homoskedasticity). If violated, use weighted least squares.</li>
<li>The errors are uncorrelated. If violated, use generalized least squares (pre-whitening)</li>
<li>The errors are <em>jointly</em> normally distributed. If violated, use robust methods or variable transformation.</li>
</ol>
</div>
<div id="geometric-interpretation" class="section level2">
<h2><span class="header-section-number">2.2</span> Geometric interpretation</h2>
<p>The respone variable is a vector in <span class="math inline">\(\mathbb{R}^n\)</span>. The fitted values <span class="math inline">\(\hat{Y} = X \hat{\beta}\)</span> span are a <span class="math inline">\(p\)</span> dimensional subspace in <span class="math inline">\(\mathbb{R}^n\)</span> when varying <span class="math inline">\(\hat{\beta}\)</span>. The least square solution is the solution for <span class="math inline">\(\hat{\beta}\)</span> such that the eucledian distance between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> is minimal. Note that the <em>true</em> <span class="math inline">\(\beta\)</span> (i.e. the one that from the data-generatling process) corresponds to the solution <span class="math inline">\(\mathbb{E}[Y]\)</span>, which also lays within the subspace, but due to the error introduced, in the process, the two do not coincide.</p>
<p><img src="figures/ols_geometry.png" width="650px" /></p>
<p>Hence, <span class="math inline">\(X\beta\)</span> is the orthogonal projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(\mathcal{X}\)</span>.</p>
<p>Note that the vector [1, …, 1] is contained in the <span class="math inline">\(p\)</span>-dimensional subspace if our model has an interecept. You can see that if you imagine we only had a model with an intercept. The design matrix would then just be such a [1, …, 1] vector. Setting <span class="math inline">\(\beta\)</span> equal to any number will yield a scaled unit vector as a prediction, i.e. just <span class="math inline">\(\beta \times [1, ..., 1]\)</span>. This subspace contains the vector [1, …, 1], since you can connect any two points in the space with a scaled version of this vector.</p>
<p>Therefore, the residuals are orthogonal to the vector [1, …, 1] , as one can see from the picture. This means <span class="math inline">\(r&#39;\textbf{1} = 0\)</span> and hence <span class="math display">\[ \frac{1}{n} \sum r&#39;\textbf{1} = \bar{r} = 0\]</span> I.e. the residuals are zero on average.</p>
</div>
<div id="hat-matrix" class="section level2">
<h2><span class="header-section-number">2.3</span> Hat matrix</h2>
<p>We can also look at another projection or mapping. <span class="math display">\[ P: \; Y \rightarrow \hat{Y}\]</span> Using <span class="math display">\[X\hat{\beta} = X(X&#39;X)^{-1}X&#39;y = Py \;\; \text{with} \;\; P = X(X&#39;X)^{-1}X&#39;\]</span> We call <span class="math inline">\(P\)</span> the hat matrix. The aforementioned projection is an orthogonal projection since:</p>
<ul>
<li><span class="math inline">\(P\)</span> is symmetric: <span class="math inline">\(P = P&#39;\)</span></li>
<li><span class="math inline">\(P\)</span> is idempotent: <span class="math inline">\(P^2 = P\)</span></li>
<li>The trace of <span class="math inline">\(P_{p\times p}\)</span> is equal to <span class="math inline">\(p\)</span></li>
</ul>
<p>The last one can be shown as follows: <span class="math display">\[ tr(X(X&#39;X)^{-1}X&#39;) = tr((X^X)^{-1}X&#39;X) = tr(I_{p \times p}) = p\]</span></p>
</div>
<div id="multiple-regression-vs.simple-regression" class="section level2">
<h2><span class="header-section-number">2.4</span> Multiple regression vs. simple regression</h2>
<p>Doing many simple regressions instead of one multiple linear regression does not give the same result in general. Let’s consider two predictors. If they are positively correlated and we omit one, we will overestimate the effect of the remaining predictor. If they are negatively correlated, we will underestimate it. Simple regressions only yield the same result as multiple linear regression if the variables are not correlated. This can be seen well algebraically.</p>
<p>Orthogonal predictors means <span class="math display">\[ (X&#39;X) = diag(\sum\limits_{i = 1}^n {x_{1i}}^2, ..., {x_{pi}}^2)\]</span> Which yields <span class="math display">\[ \beta_j = \sum\limits_{i = 1}^n x_{ij} Y_i / \sum\limits_{i = 1}^n {x_{ij}}^2\]</span> So we can easily see that the coefficient <span class="math inline">\(\beta_j\)</span> only depends on the j-th predictor.</p>
</div>
<div id="properties" class="section level2">
<h2><span class="header-section-number">2.5</span> Properties</h2>
<p><img src="figures/ols_properties.jpg" width="1224" /></p>
</div>
<div id="tests" class="section level2">
<h2><span class="header-section-number">2.6</span> Tests</h2>
<p>We know that, if $_p(0, ^2) the distribution of our coefficients is as follows: <span class="math inline">\(\hat{\beta} \sim \mathcal{N}_p(\beta, \sigma^2(X&#39;X)^{-1})\)</span>. Hence, we know that the standardized versions of the coefficients, i.e. <span class="math display">\[\frac{\hat{\beta}_j - \beta_j}{\sigma^2(X&#39;X)_{jj}} \sim \mathcal{N}_p(0, 1)\]</span></p>
<p>If our Null Hypothesis is <span class="math inline">\(\beta_j = 0\)</span> the above becomes <span class="math display">\[\frac{\hat{\beta}_j}{\sigma^2(X&#39;X)_{jj}} \sim \mathcal{N}_p(0, 1)\]</span> We can plug in our estimate for <span class="math inline">\(\sigma^2\)</span>,</p>
<p><span class="math display">\[\frac{\hat{\beta}_j}{\hat{\sigma}^2(X&#39;X)_{jj}} \sim \mathcal{t}_{n-p}(0, 1)\]</span></p>
<p>Individual tests measure the significance of a variable given all the other variables in the model. If you want to look at whether a group of variables is significant, i.e. your Null Hypothesis is <span class="math inline">\(H_0: \beta_1 = ... = \beta_k = 0\)</span></p>
<p>If you recall the picture from above, we can see that we can decompose the total sum of squares into <span class="math display">\[ SST = SSR + SSE\]</span></p>
<p>The F-ratio is nothing else than the share of variance we can explain with our mode, scaled by some degree of freedom related stuff.</p>
<p><span class="math display">\[F = \frac{\|\hat{Y} - \bar{Y}\|^2 / (p-1)}{\|Y - \bar{Y}\|^2/(n-p)}\]</span></p>
<p>For the goodness of fit measure <span class="math inline">\(R^2\)</span> drop those scaling factors.</p>
<p><span class="math display">\[R^2 = \frac{\|\hat{Y} - \bar{Y}\|^2}{\|Y - \bar{Y}\|^2}\]</span></p>
<p>Similarly to t-tests derived above, we can construct confidence intervals. <span class="math display">\[\hat{\beta}_j ± \sqrt{\hat{\sigma}^2 {(X&#39;X)_{jj}}^{-1}} * t_{n-p, 1-\alpha/2}\]</span></p>
</div>
<div id="diagnostics" class="section level2">
<h2><span class="header-section-number">2.7</span> Diagnostics</h2>
<p>The Tuskey-Anscombe plot is a good tool to detect model violations. It takes advantage of the fact that the residuals are never correlated with the predictors and we can display the response versus the residuals also for <span class="math inline">\(p&gt;1\)</span>, which is not possible if we plot against the predictors (at least not in one plot). Ideally, the points fluctuate randomly around the horizontal zero-line.</p>
<p>The Normal-plot helps to identify violations of the normality assumption.</p>
<p>You can also plot the residuals against time or calculate auto-correlation to detect serial dependency in the residuals.</p>
</div>
<div id="generalized-least-squares" class="section level2">
<h2><span class="header-section-number">2.8</span> Generalized least squares</h2>
<p>Applies to the situation where</p>
<p><span class="math display">\[ Y = X\beta + \epsilon \;\;\; |\; \epsilon \sim \mathcal{N}(0, \Sigma)\]</span> Where <span class="math inline">\(\Sigma\)</span> takes a more general form than previously:</p>
<ul>
<li>The errors can be correlated (so <span class="math inline">\(\Sigma\)</span> does not have to be diagonal matrix)</li>
<li>The errors do not need to have identical variance (homoscedasticity)</li>
</ul>
<p>We can transform the data with a matrix <span class="math inline">\(A\)</span> such that we end up with <span class="math display">\[ \tilde{Y} = \tilde{X}\beta + \tilde{\epsilon} \;\;\; |\; \tilde{\epsilon} \sim \mathcal{N}(0, \bf{1_{p \times p}})\]</span> The following must hold</p>
<p><span class="math display">\[ 
\begin{split}
Cov(A \epsilon) &amp;:= \bf{1_{n \times n}} \\
\mathbb{E}[A\epsilon \epsilon&#39; A&#39;] &amp; = A \Sigma A&#39; \;\; \;\;\;\;| \;\text{decomposing} \; \Sigma \; \text{e.g. with Cholesky} \;\; \\
&amp;= ACC&#39;A&#39; \;\; | \text{now choosing}\; A = C^{-1} \\
&amp; = \bf{1_{n \times n}} \\
\end{split}
\]</span> Hence, we transform the data with the inverse of the (for example) lower triangular matrix resulting from the Cholesky decomposition and we will get uncorrelated errors.</p>
<p>A special case is when we have uncorrelated errors, but heteroskedasticity. Choose the weight <span class="math inline">\(\tilde{w}\)</span> such that <span class="math inline">\(Var(\tilde{w}_i x_i) = 1\)</span>. If <span class="math inline">\(Var(x_i) = {\sigma_i}^2\)</span> set <span class="math inline">\(\tilde{w}_i = 1 / {\sigma_i}\)</span> to fulfill the above equation. The optimization problem becomes</p>
<p><span class="math display">\[
\begin{split}
\hat{\beta} &amp;= \arg\min\limits_{\beta} \sum {\tilde{r}_i}^2 =
\arg\min\limits_{\beta} \sum (\tilde{w}_i r_i)^2 \\
&amp;=\arg\min\limits_{\beta} \sum w {r_i}^2 = 
\arg\min\limits_{\beta} \sum w {(y_i - {x_i}&#39;\beta)}^2
\end{split}
\]</span> So <span class="math inline">\(w_i = \tilde{w_i}^2 = 1/ \sigma_i^2\)</span> is just a weighted regression.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-parametric-density-estimation.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
